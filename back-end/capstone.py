# -*- coding: utf-8 -*-
"""Capstone.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16RUW8MhDTvLrgQkcYfff53zap2LYA4sO
"""

#probably should edit this code here to run local version of your dataset

from google.colab import drive
drive.mount('/content/drive')







import pandas as pd
  
# od.download(
    # "https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text")

# path = "/content/emotion-detection-from-text/tweet_emotions.csv"

path ="/content/drive/MyDrive/Capstone/NUS 2022 capstone data/sensitised_IG_RnR_datasets_unlabelled.xlsx"
df = pd.read_excel(path)
df.head()

df.shape

df["text"] = df["text"].str.lower()
df["text"] = df["text"].astype(str)
df["text"] = df["text"].replace("na", None)

df.isna().sum()

data_text = df[~df["text"].isna()][['text']]

data_text['index'] = data_text.index
documents = data_text

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer 
from nltk.stem.porter import *
import numpy as np
np.random.seed(2018)
import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

"""- All stopwords are removed
- Words that have fewer than 3 characters are removed.
- Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present.
- Words are stemmed — words are reduced to their root form.
"""

added_stopwords = ["http", "bgaabha"] # to be changed based on context
def lemmatize_stemming(text):
  stemmer = SnowballStemmer(language='english')
  #It is a stemming algorithm which is also known as the Porter2 stemming algorithm as 
  #it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.
  return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
    result = []
    text=str(text)
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and token not in added_stopwords and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result

documents

#data cleaning

documents["text"] = documents["text"].str.replace(r"(@\[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?",'', regex=True)

# example
doc_sample = documents[documents['index'] == 1320].values[0][0]
print('original document: ')
words = []
for word in doc_sample.split(' '):
    words.append(word.lower())
print(words)
print('\n\n tokenized and lemmatized document: ')
print(preprocess(doc_sample))

processed_docs = documents["text"].map(preprocess)
processed_docs

from wordcloud import WordCloud
import matplotlib.pyplot as plt

comment_words = ''

for val in processed_docs:
   
     
    comment_words += " ".join(val)+" "
 
wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = gensim.parsing.preprocessing.STOPWORDS,
                min_font_size = 10).generate(comment_words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

#maps each word to their unique ID

dictionary = gensim.corpora.Dictionary(processed_docs)
count = 0
for k, v in dictionary.iteritems():
    print(k, v)
    count += 1
    if count > 10:
        break

"""Filter out tokens that appear in
- less than 15 documents (absolute number) or
- more than 0.5 documents (fraction of total corpus size, not absolute number). Tokens that appear in more than 50% of the total corpus are also removed as default.
- after the above two steps, keep only the first 100000 most frequent tokens. Set to ‘None’ if you want to keep all.

"""

dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)

"""create a dictionary reporting how many
words and how many times those words appear. Save this to ‘bow_corpus’
"""

processed_docs[8]

bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]
bow_corpus[2]

bow_doc_4310 = bow_corpus[8]
for i in range(len(bow_doc_4310)):
    print("Word {} (\"{}\") appears {} time.".format(bow_doc_4310[i][0], 
                                                    dictionary[bow_doc_4310[i][0]],
                                                    bow_doc_4310[i][1]))

"""TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents."""

from gensim import corpora, models
tfidf = models.TfidfModel(bow_corpus)
corpus_tfidf = tfidf[bow_corpus]
from pprint import pprint
# preview TF-IDF scores for our first document
for doc in corpus_tfidf:
    pprint(doc)
    break

"""Running LDA using Bag of Words

Parameters for LDA model in gensim
Following are the important and commonly used parameters for LDA for implementing in the gensim package:

The corpus or the document-term matrix to be passed to the model (in our example is called doc_term_matrix)
Number of Topics: num_topics is the number of topics we want to extract from the corpus.
id2word: It is the mapping from word indices to words. Each of the words has an index that is present in the dictionary.
Number of Iterations: it is represented by Passes in Python. Another technical word for iterations is ‘epochs’. Passes control how often we want to train the model on the entire corpus for convergence.
Chunksize: It is the number of documents to be used in each training chunk. The chunksize controls how many documents can be processed at one time in the training algorithm.
"""

lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=6, id2word=dictionary, passes=2, workers=2)

for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

"""When looking at the coherence using the C_umass or C_v algorithm, the best is usually the max."""

# Log Likelyhood: Higher the better
print("Log Likelihood: ", lda_model.score(bow_corpus))

# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)
print("Perplexity: ", lda_model.perplexity(bow_corpus))
#perplex=model.log_perplexity(corpus, total_docs=len(corpus))



topics = []
score = []
for i in range(1,20,1):
   lda_model = gensim.models.LdaMulticore(corpus=bow_corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)
   cm = gensim.models.CoherenceModel(model=lda_model, texts = processed_docs, corpus=bow_corpus, dictionary=dictionary, coherence='c_v')
   topics.append(i)
   score.append(cm.get_coherence())
_=plt.plot(topics, score)
_=plt.xlabel('Number of Topics')
_=plt.ylabel('Coherence Score')
plt.show()

topics = []
score = []
for i in range(1,20,1):
   lda_model = gensim.models.LdaMulticore(corpus=bow_corpus, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)
   cm = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, dictionary=dictionary, coherence='u_mass')
   topics.append(i)
   score.append(cm.get_coherence())
_=plt.plot(topics, score)
_=plt.xlabel('Number of Topics')
_=plt.ylabel('Coherence Score')
plt.show()

vectorizer = CountVectorizer(analyzer='word',       
                             min_df=10,                        # minimum reqd occurences of a word 
                             stop_words='english',             # remove stop words
                             lowercase=True,                   # convert all words to lowercase
                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3
                             # max_features=50000,             # max number of uniq words
                            )

data_vectorized = vectorizer.fit_transform(documents["text"])

# Materialize the sparse data
data_dense = data_vectorized.todense()

# Compute Sparsicity = Percentage of Non-Zero cells
print("Sparsicity: ", ((data_dense > 0).sum()/data_dense.size)*100, "%")

from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from pprint import pprint

# Define Search Param
search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}

# Init the Model
lda = LatentDirichletAllocation()

# Init Grid Search Class
model = GridSearchCV(lda, param_grid=search_params)

# Do the Grid Search
model.fit(data_vectorized)

# Best Model
best_lda_model = model.best_estimator_

print("GridSearch:")

# Model Parameters
print("Best Model's Params: ", model.best_params_)

# Log Likelihood Score
print("Best Log Likelihood Score: ", model.best_score_)

# Perplexity
print("Model Perplexity: ", best_lda_model.perplexity(data_vectorized))



import pyLDAvis.gensim_models
pyLDAvis.enable_notebook()# Visualise inside a notebook

lda_display = pyLDAvis.gensim_models.prepare(lda_model, bow_corpus, dictionary)
pyLDAvis.display(lda_display)



"""Running LDA using TF-IDF

"""

topics = []
score = []
for i in range(1,20,1):
   lda_model = gensim.models.LdaMulticore(corpus=corpus_tfidf, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)
   cm = gensim.models.CoherenceModel(model=lda_model, texts = processed_docs, corpus=bow_corpus, dictionary=dictionary, coherence='c_v')
   topics.append(i)
   score.append(cm.get_coherence())
_=plt.plot(topics, score)
_=plt.xlabel('Number of Topics')
_=plt.ylabel('Coherence Score')
plt.show()

topics = []
score = []
for i in range(1,20,1):
   lda_model = gensim.models.LdaMulticore(corpus=corpus_tfidf, id2word=dictionary, iterations=10, num_topics=i, workers = 4, passes=10, random_state=100)
   cm = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, dictionary=dictionary, coherence='u_mass')
   topics.append(i)
   score.append(cm.get_coherence())
_=plt.plot(topics, score)
_=plt.xlabel('Number of Topics')
_=plt.ylabel('Coherence Score')
plt.show()

lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=6, id2word=dictionary, passes=2, workers=4)
for idx, topic in lda_model_tfidf.print_topics(-1):
    print('Topic: {} \nWord: {}'.format(idx, topic))

lda_display = pyLDAvis.gensim_models.prepare(lda_model_tfidf, corpus_tfidf, dictionary)
pyLDAvis.display(lda_display)



